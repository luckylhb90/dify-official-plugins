model: us.amazon.nova-pro-v1:0
label:
  en_US: Nova Pro V1 (US.Cross Region Inference)
icon: icon_s_en.svg
model_type: llm
features:
  - agent-thought
  - tool-call
  - stream-tool-call
  - vision
model_properties:
  mode: chat
  context_size: 300000
parameter_rules:
  - name: enable_cache
    label:
      zh_Hans: 启用提示缓存
      en_US: Enable Prompt Cache
    type: boolean
    required: false
    default: true
    help:
      zh_Hans: 启用提示缓存可以提高性能并降低成本。Amazon Nova Pro支持在system和messages字段中使用缓存检查点，每个检查点至少需要1024个token。
      en_US: Enable prompt caching to improve performance and reduce costs. Amazon Nova Pro supports cache checkpoints in system and messages fields with a minimum of 1024 tokens per checkpoint.
  - name: max_new_tokens
    use_template: max_tokens
    required: true
    default: 2048
    min: 1
    max: 5000
  - name: temperature
    use_template: temperature
    required: false
    type: float
    default: 1
    min: 0.0
    max: 1.0
    label:
      zh_Hans: 生成内容的随机性。
      en_US: The amount of randomness injected into the response.
  - name: top_p
    required: false
    type: float
    default: 0.999
    min: 0.000
    max: 1.000
    label:
      zh_Hans: 在核采样中，Amazon Nova 按概率递减顺序计算每个后续标记的所有选项的累积分布，并在达到 top_p 指定的特定概率时将其切断。您应该更改温度或top_p，但不能同时更改两者。
      en_US: In nucleus sampling, Amazon Nova computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.
  - name: top_k
    required: false
    type: int
    default: 0
    min: 0
    # tip docs from aws has error, max value is 500
    max: 500
    label:
      zh_Hans: 对于每个后续标记，仅从前 K 个选项中进行采样。使用 top_k 删除长尾低概率响应。
      en_US: Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.
pricing:
  input: '0.0008'
  output: '0.0032'
  unit: '0.001'
  currency: USD
